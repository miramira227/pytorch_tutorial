{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Autograd: Automatic Differentiation",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luX8MfcNL0Hs",
        "colab_type": "text"
      },
      "source": [
        "**2. Autograd: Automatic Differentiation**\n",
        "\n",
        "The ``autograd`` package provides automatic differentiation for all operations on Tensors. \n",
        "\n",
        "If you set its attribute ``.requires_grad`` as True, it starts to track all operations on it. When you finish your computation you can call ``.backward()``and have all the gradients computed automatically. The gradient for this tensor will be accumulated into ``.grad`` attribute. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZIJA4NkLp84",
        "colab_type": "code",
        "outputId": "06e6af77-1e1f-41ab-b77b-71e809cd1d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import torch \n",
        "\n",
        "x = torch.ones(2, 2, requires_grad = True)\n",
        "print(x)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 1.],\n",
            "        [1., 1.]], requires_grad=True)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bXPLjRgjaPkV",
        "colab_type": "code",
        "outputId": "90a20a5d-d1df-46e1-88ec-21cbab0c26ab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "y = x + 2\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[3., 3.],\n",
            "        [3., 3.]], grad_fn=<AddBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LSJXtRKwaSo6",
        "colab_type": "code",
        "outputId": "df7a0a4b-f742-486d-97c3-c3df4d085cc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(y.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<AddBackward0 object at 0x7ff097d40a58>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2Pih51GaYyN",
        "colab_type": "code",
        "outputId": "4acfcdcc-f949-4112-a537-ccc78ea38a3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "z = y * y * 3\n",
        "out = z.mean()\n",
        "\n",
        "print(z, out)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[27., 27.],\n",
            "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHz1nZmQbCKI",
        "colab_type": "code",
        "outputId": "dcc03a58-d351-44ad-fa6d-8c3ca8bbfe29",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# .requires_grad_(...) changes an existing Tensor's requires_grad flag in-place. \n",
        "\n",
        "a = torch.randn(2, 2)\n",
        "a = ((a * 3)/(a - 1))\n",
        "print(a.requires_grad)\n",
        "\n",
        "a.requires_grad_(True)\n",
        "print(a.requires_grad)\n",
        "\n",
        "b = (a * a).sum()\n",
        "print(b.grad_fn)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "False\n",
            "True\n",
            "<SumBackward0 object at 0x7ff096ce4978>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epwmutrVckeB",
        "colab_type": "text"
      },
      "source": [
        "**Gradients**\n",
        "\n",
        "Let's backprop now Because ``out`` contains a single scalar, ``out.backward()`` is equivalent to ``out.backward(torch.tensor(1)).``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ns3QSKSXchGp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "out.backward()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_4vQ7SS1c9SQ",
        "colab_type": "code",
        "outputId": "a5f4bb53-21bc-4892-f044-b071477c9757",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[4.5000, 4.5000],\n",
            "        [4.5000, 4.5000]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwVD6MH-kF3N",
        "colab_type": "code",
        "outputId": "d6e1f509-c7a3-42ad-b096-98f785d5392e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = torch.randn(3, requires_grad = True)\n",
        "\n",
        "y = x * 2\n",
        "while y.data.norm() < 1000:\n",
        "    y = y * 2\n",
        "\n",
        "print(y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([  761.4349, -1168.1293,   353.6544], grad_fn=<MulBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrW_-k9vmUWs",
        "colab_type": "code",
        "outputId": "a8ae9616-dbff-40fa-f384-00da42a7f98a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gradients = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\n",
        "y.backward(gradients)     # gradient에는 output결과값과 동일한 shape이 들어가야 한다. \n",
        "\n",
        "print(x.grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([5.1200e+01, 5.1200e+02, 5.1200e-02])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtTuSVZmtsnU",
        "colab_type": "code",
        "outputId": "a974e44f-53f4-42ea-f109-b86b80ebc6d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# to stop autograd from tracking history on Tensors: wrap the code block in with torch.no_grad():\n",
        "\n",
        "print(x.requires_grad)\n",
        "print((x ** 2).requires_grad)\n",
        "\n",
        "with torch.no_grad():\n",
        "    print((x ** 2).requires_grad)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n",
            "False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2p6PU9qohYq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}